%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University/School Laboratory Report
% LaTeX Template
% Version 3.1 (25/3/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Linux and Unix Users Group at Virginia Tech Wiki 
% (https://vtluug.org/wiki/Example_LaTeX_chem_lab_report)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt,a4paper]{article}

%\usepackage[version=3]{mhchem} % Package for chemical equation typesetting
%\usepackage{siunitx} % Provides the \SI{}{} and \si{} command for typesetting SI units
\usepackage{graphicx} % Required for the inclusion of images
\usepackage{geometry}
%\usepackage{natbib} % Required to change bibliography style to APA
%\usepackage{amsmath} % Required for some math elements 
%\usepackage{enumerate}
\usepackage{indentfirst}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
%  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}
%\usepackage{biblatex}
%\addbibresource{report.bib}
%\setlength\parindent{24pt}


%\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Improvements for Congestion Control Algorithms in Large-Scale Datacenter Network \\ Algorithm Adjustment for DCQCN \\ undergraduate dissertation} % Title

\author{Yuchen \textsc{Yang}} % Author name

\date{\today} % Date for the report

\begin{document}

\maketitle % Insert the title, author and date

\begin{center}
\begin{tabular}{l}
141242048 \\
Kuang Yaming Honors School, Nanjing University \\
ranchyang96@gmail.com \\
Instructor: Professor Chen Tian
\end{tabular}
\end{center}


\begin{abstract}
	Modern datacenters are experiencing a fierce increase in scale and traffic bandwidth.
	Remote Direct Memory Access (RDMA) permits high-throughput, low-latency networking,
	which is especially useful in such a large-scale scenario.
	The most primitive method (like TCP/IP stack) for congestion control is to drop packets when the receiver buffer is full.
	Later we have Piriority-based Flow Control (PFC) to generate congestion information in ACKs.
	Before this paper, we have Datacenter Quantized Congestion Notification (DCQCN) which uses the state-of-the-art scheme for congestion control.
	However, we may find DCQCN incapable of some large-scale traffic scenarios.
	In this paper, we analyze the drawbacks of DCQCN.
	At the same time, we present our improvements to DCQCN and name it DCQCN+.
	Improvements mainly focus on the adaptive increasing step and intervals.
	We have implemented them on testbeds and NS3 simulation.
	Our method has 10 times smaller latency than DCQCN under large-scale conditions (incast of over 400:1) and 4 times larger flow capability than DCQCN.
	While in small incast cases, we have similar performance with DCQCN.
	This paper additionally focuses on my own work, about the testbed implementation and configurations of the method.
	It actually includes many detailed methods used for execution of experiments and detailed information about testbed experiment results.
\end{abstract}

\tableofcontents

\section{Introduction}

Modern datacenters are experiencing a fierce increase in both scale and bandwidth.
To be more specific, we find following features of datacenters nowadays:
\begin{enumerate}
	\item Small latency: $<100\mu s$
	\item High bandwidth: $10/40~100Gbps$
	\item Shallow buffer: $<300MB$ for ToR
	\item Large scale: $>10000$ machines
\end{enumerate}
As we know, datacenters need small RTTs (some may be even tens of microseconds),
so abilities to handle burst flows for incasts and support the equality and balancing for concurrent flows matter a lot.
Traditional TCP/IP stack surely can't do the job.
When encountering a large number of concurrent flows, TCP may choose to drop packets when the receiver buffer is full.
That's not tolerable for sure in datacenters since datacenters require lossless network to ensure the security and accuracy in large-scale traffic.
Then the first idea coming is to find a way to predict the congestion of the receiver end.
We hope to let the sender get some information about the receiver.

Then we get Explicit Congestion Notification (ECN) \cite{ECN} at the beginning.
This method uses a mark in the IP header of the ACK.
If the receiver dropped a packet, it echoes the congestion indication to the sender, so that the sender can reduce its sending rate.
Such ECN flags indicate the existance of congestion from receiver side after congestions have already happened.
ECN somehow relieves the congestion and makes the receiver side drop fewer packets, but this still can't be lossless.

Later Quantized Congestion Notification (QCN) \cite{QCN} is developed.
QCN enables the switch to control the packet sending rate of an Ethernet source whose packets are traversing the switch.
This maintains a stable queue occupancy and is easy to be implemented on hardwares.
QCN is also applicable for multiple flows in a same port.

Priority-based Flow Control (PFC) \cite{PFC} applies the PAUSE frame to make things better.
PAUSE is used by the receiver to send feedback to the sender about the remaining buffer space.
PFC actually divide services into 8 classes to make the feedback information more accurate.
Since the PAUSE frame carries more information, sender can know the exact remaining space in receiver buffer.
Thus the reaction on sending rate can be more accurate.
However this method can't be specific on flows but only on ports, which is usually combined with ECN to make it work better.

Datacenter TCP (DCTCP) \cite{dctcp} begins to react in proportion to the extent of congestion instead of presence.
This congestion level is conveyed by ECN marks carried by packets.
These ECN marks are based on instantaneous queue length.

Before this paper, we have Datacenter QCN (DCQCN) \cite{dcqcn} which handles congestion better and is the basic point of our work.
DCQCN is the application of QCN on datacenters, which divides the overall algorithms into three parts.
It's kind of combination of Datacenter TCP (DCTCP) and PFC.
A brandnew concept mentioned here is Congestion Notification Packet (CNP), which functions similarly as PAUSE frame in PFC but carries more informations.
CNP packets are actually sent by NP which is the receiver end.
Each time when a packet with ECN mark is got and there are no CNP packets sent during the last interval period, a CNP is sent to notify the
sender.
Here the interval period mentioned is usually a interval set up in advance by hardware to make sure that there won't be a CNP burst on
RP side.
This interval is also greatly limited by hardware ability.
Detailed information is also be mentioned in later sections.

In the end here is a brief introduction of the most important points about our new method DCQCN+.
DCQCN+ deploys dynamic rate control mechanisms to adapt to incast of different scales instead of the fixed rate control for DCQCN.
What I'm in charge of is the testbed configuration and experiments.

The paper is organized as follows.
In Section 2 we introduce the background of the project.
In Section 3 we present limitations of DCQCN and necessity for improvements.
In Section 4 we show possible improvements for DCQCN.
In Section 5 I display detailed methods used for testbed experiments.
In Section 6 we show the results of experiments.
In Section 7 there are some discussions and future work.

\section{Background}
\subsection{Mellanox switch and ConnectX-4}
In the testbed experiments, we are using Mellanox SN2700 switch and ConnectX-4 Network Interface Cards.
Mellanox SN2700 carries a huge throughput of 6.4Tb/s, 32 ports at 100GbE.
The port speed can actually vary from 10Gb/s, 25Gb/s, 40Gb/s and 100Gb/s.
All 32 ports are connected to 16 machines inside our testbed with 2 each, but only 9 ports on 9 separate machines are used inside our
testbed experiments.

ConnectX-4 EN adapter can support 100Gbps Ethernet Connectivity.
Its Virtual Protocol Interconnect also supports EDR 100Gbps InfiniBand traffic.
We have ConnectX-4 adapters on all machines in our testbed.
Additionally we have tested the ability of ConnectX-5 adapters on a new testbed about the Congestion Notification Packet generation
ability.
This is described in later sections.

\subsection{InfiniBand}

InfiniBand is a computer-networking communication standard used in high-performance computing.
Such standard supports features of high throughput and low latency.

It can actually work both among and within computers. It's most commonly used in supercomputers and data centers.

In later testbed experiments, commands ''ib\_send'' includes IB which is the abbreviation of InfiniBand.

\subsection{Remote Direct Memory Access}
Remote Direct Memory Access almost satisfies all the demands for datacenter networks.
It permits high throughput and low latency.
Similar to Direct Memory Access, RDMA actually allows user-space applications to directly read or write without the operation from
any operating systems.
Such network feature omits the possible copies inside systems, thus performs better inside datacenters.

\subsection{RoCE v2}

The complete name is RDMA over Converged Ethernet version 2 \cite{RoCEv2}.
RoCE is a network protocol that allows RDMA over an Ethernet network
which has advantages of low latency, low CPU usage and high bandwidth.

RoCE v1 is an Ehternet link layer protocol which allows traffic under a same Ethernet domain.
RoCE v2 is an Internet layer protocol which permits routing.
We are using RoCE v2 to make improvements in Internet layer.

\subsection{Priority-based Flow Control}
Priority-based Flow Control ensures the lossless feature for RDMA.
No buffer packet overflow is a must for datacenter networks.

However, PFC actually does nothing to make the latency low.
When the receiver buffer is almost full, the packets inside the buffer queue up for a long time which makes the queueing latency
extremely high.
Such unfairness and high latency are fatal to datacenters and we hope to make up the drawbacks.

\subsection{Explicit Congestion Notification}
Explicit Congestion Notification is designed to indicate congestion to the sender.
An ECN-aware router may set a mark in the IP header instead of dropping a packet in order to signal impending congestion.
The receiver of the packet echoes the congestion indication to the sender,
which reduces its transmission rate if it detects a dropped packet.

\subsection{Datacenter TCP}

DCTCP has 2 brandnew features. It somehow reveals the basic points of DCQCN.

The first is reacting in proportion to the extent of congestion but not its presence.
It's actually reducing window size based on fraction of marked packets.

The second is that ECN marks are based on instantaneous queue length.
It provides fast feedback to better deal with bursts and also simplifies the hardware implementation.

\subsection{DCQCN}
Datacenter Quantized Congestion Notification is explained in detail by \cite{dcqcn}.
Here I wil generally describe the mechanisms used in DCQCN.

Three parts of algorithms are developed.
The parts are Congestion Point (CP), Reaction Point (RP) and Notification Point (NP),
which are switches, senders and receivers respectively.
Both the switch and end points participate in to make things right.

For NP, i.e. receivers, they need to generate and send CNP when a packet with ECN mark and there is no CNP sent during the last interval.
CNPs are generated by receivers, indicating the remaining size of the receiving buffer.

For CP, a function of ECN marking probability P is related with Egress Queueing size S:

for $0<S<K_{min}$, $P = 0$

for $K_{min}<S<K_{max}$, $P = (S - K_{min})/(K_{max} - K_{min})*P_{max}$

for $S>K_{max}$, $P = 1$

which is shown in Figure ~\ref{fig:ECNMarking}.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=3in]{ECNMarking}
		\caption{ECN Marking Probability for CP in DCQCN}
		\label{fig:ECNMarking}
	\end{center}
\end{figure}

Such marking probability is a brandnew idea from QCN.
Previously marking ECN or not is a fixed thing without possibility.
Such 0/1 resolution for ECN is not proper because that may cause the late reaction from sender.

\subsection{Python Paramiko Library}

The mostly used library in my scripts is Python Paramiko
which provides secure connection for remote command execution and file transmission.
It's actually a Python implementation of SSHv2 protocol, providing functions of both server and client.
To be more specific, I contracted them down to four functions, one for connection checking,
one for remote command execution and the other two for file transmission between local and remote machines.
Every remote command execution here is using SSH protocol and thus my functions can be implemented by Python Paramiko.

Apart from the basic points, another tough point is fetching responses of command executions.
We need to record standard output and sometimes error of the command execution.
In Python Paramiko, it's packed as the return value of ''exec\_command''.
We fetch the responses from ''readlines'', both error messages and output messages can be collected.

When printing these responses on screen, we filter the necessary messages out.
I also set to print specific information to files for later checks.

\subsection{Python Multithreading}

This point is more common for other programming cases.

In the original version, the time for running is previously estimated.
Thus the execution time of the program is greatly increased or a command is run before its previous steps are finished.
Function ''join'' can correctly sequence all the steps in the program.

In my scripts, there are 5 stages divided and each is ensured to be completed with a series of ''join''.
Thus we don't need to estimate the wait time and can safely wait and watch possible error messages printed on the screen.


\section{Problems of DCQCN}

The major problem for DCQCN is the failure of dealing with large-scale incast traffic.
Both in simulation \cite{NS3} and testbed experiments show that the switch buffer queue length remains high
after a large burst at the beginning.

\subsection{Testbed experiments}

We use the topology from Figure ~\ref{fig:Topology}.
Actually there are 9 hosts and one of them is the receiver, the rest of them are senders.
For large-scale congestion, we use Tcpdump \cite{Tcpdump} to capture packets for throughput statistics.
We also turn on sniffer on NICs to make sure that traps are triggered in Lipcap.

In testbed experiments, we actually find that when the flow number is under 400, the queue length will go down after 2 or 3 seconds.
Here the flows are continuous InfiniBand traffic lasting for about 80 seconds and they are started at approximately the same time.
There could be some difference in starting times but within 1 second.
This means that when the flow number is not large, convergence should be reached within 2 seconds.

However, when we start over 400 flows, the high queue length lasts till the end of all flows.
Convergence fails to be reached when the flow number gets really large.

Nowadays the tendency is that scale of datacenters grow larger and larger which gives much pressure on datacenter networks.
Such a long queue length surely lead to high network latency and unfairness.
Thus the demand of datacenter network is not met.

\subsection{NS3 Simulation}

Here we use the DCQCN simulation \cite{DCQCNsim} released by Yibo Zhu, the designer of DCQCN.

In simulation results, we find even worse convergence effect.
Here from Figure ~\ref{fig:DCQCNfail}, we can see that 160 flows fail to be converged under 10Gbps links and 80 flows fail to 
be converged under 40Gbps links.
All the parameters used here are default parameters on Mellanox official forum \cite{MellanoxOfficial}.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=6in]{DCQCNfail}
		\caption{DCQCN fails to converge}
		\label{fig:DCQCNfail}
	\end{center}
\end{figure}

\subsection{Difference between DCQCN and ConnectX-4 implementation}

When tested in testbeds, we find something really strange.

When we set all the links to 10Gbps in a topology shown in Figure ~\ref{fig:Topology},
we can see that at the beginning the sending rate increases at line rate.
However when we change the link speed to 40Gbps or 100Gbps with the same topology,
we see the speed increasing tendancy similar to the 10Gbps one, which is not at line rate.

There must be some implementation difference between the ConnectX-4 and the original DCQCN.
I guess the possible reason for that is, at the creation time of DCQCN, 10Gbps is a suitable upper bound of line rate start.
10Gbps is also a maximal flow rate under DCQCN's control, thus the speed gaining scheme can be suitable for a long time.

\section{Improvements for DCQCN}

The key factor of DCQCN's failure is DCQCN's increase step, recovery speed and CNP supplies.
These information used are out of date for the growing scalability for data centers.
Those fixed increase steps, recovery speed and CNP intervals are not adaptive for large-scale traffic.
What we do is to find factors which actually influence the congestion and can predict the congestion level better.

Our work, DCQCN+ uses dynamic parameters to better analyze the congestion and therefore relieve it.
DCQCN never cares about the incast scale and thus uses fixed parameters to do the job.
That surely won't work for large-scale incast since the reaction and adjustments of sending rate are not very accurate.

DCQCN+ is designed to use the CNP period carried in CNPs and the flow rate to reflect the condition of the incast scale.
There are some remaining available fields to carry the period information of CNP.
Also, since larger flows are more probable to be marked with ECN, its possibility to trigger a CNP is larger.
Thus all congested flows can get similar rates and be balanced.

To be more specific, improvements are discussed from three points (Congestion Point, Notification Point and Reaction Point).

\subsection{Congestion Point}

This part remains almost the same.
What we can do at Congestion Point is really limited since CP is responsible for ECN marking.
The probability for ECN marks are very resonable so we leave this part the same.

We also need to mention here that PFC is still enabled to ensure lossless feature of datacenters.
Although our method can make sure that convergence can be reached in most cases, we can't ensure lossless feature at the starting burst
if PFC is disabled.

\subsection{Notification Point}

Receivers are in charge of generating CNPs.
This is the most important part of the algorithm.

At NP, two factors decide the CNP interval, the hardware ability of generating CNP and the demand for CNP at Reaction Point.
We hope to supply enough CNPs with shorter intervals for RP to properly adjust the sending rate.
At the same time we don't want to cause other problems like CNP burst or CNP congestion if we set the interval which is too short.
Such problems may cause unnecessary bandwidth cost.
The ability of hardware may influence the minimal interval and demands for CNPs decide the maximal interval.
We prefer the maximal interval to relieve the possibility of bandwidth cost.

Once the flow rate collects enough information to decide the incast scale, the timer period should be decided.

\subsection{Reaction Point}

Here we first show the original DCQCN RP algorithm pseudocode in Figure ~\ref{fig:RPalg}.
\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=4in]{RPalg}
		\caption{RP algorithm from DCQCN}
		\label{fig:RPalg}
	\end{center}
\end{figure}

We make some changes on the updates of parameters.

The reaction when receiving CNP is similar:
\[R_T=R_C\]
\[R_C=R_C(1-\alpha /2)\]
\[\alpha = (1-g)\alpha + g\]

Here $\alpha$ is a reduction factor, actually indicates the congestion level at the CP.
$R_C$ denotes current rate and $R_T$ denotes target rate.
$g$ is a parameter to estimate the congestion level.

Instead of DCQCN's fixed rate increase timer $K=55\mu s$, we make it flexible:
\[K=\lambda max(\tau, \frac{MTU}{R_C})\]
where $MTU$ denotes Maximal Transmit Unit and $\tau$ denotes CNP interval.

Here we remove the byte counter and add the rate increase counter.
When the rate increase counter times out, the state S increses 1.

The fast recovery scheme remains the same when $S$ is less than the threshold $F = 5$:
\[R_C=\frac{R_C+R_T}{2}\]

When $F<S<4F$,
\[R_T=R_T+ min(\frac{1}{10}R_C,\frac{1}{100}R_l)\]
\[R_C=\frac{R_C+R_T}{2}\]
where $R_l$ is a ratio used to bound the increase step for small incast cases.

When $S>4F$, Hyper Increase is applied,
\[R_T=R_T+min(R_C, \frac{S-4F}{100}R_l)\]
\[R_C=\frac{R_C+R_T}{2}\]

Additionally, when $\alpha$ timer runs out,
\[\alpha=(1-g)\alpha\]

We can see a lot of changes to make the algorithm better.
Such dynamic adjustments of parameters greatly improve the buffer estimation in advance and drain the congestion fast
to maintain the buffer queue length low.

\section{Implementation}

We set up the testbed of testing for DCQCN+ on Mellanox SN2700 switch and 9 Ubuntu servers with ConnectX-4 adapter cards.
Mellanox SN2700 provides the most predictable,
highest density 100GbE switching platform for the growing demands of today's data centers,
which can easily satisfy our experiment environment for over 400 flows at a time.
ConnectX-4 dual-port adapter cards with Virtual Protocol Interconnect (VPI) support 100Gb/s InfiniBand and 100Gb/s Ethernet connectivity.
All the features including RoCE v2 \cite{RoCEv2}, PFC and ECN are supported on Mellanox SN2700 and ConnectX-4 adapter card.

To generate enough number of flows for our experiment, the topology is shown in Figure ~\ref{fig:Topology}.
Each machine can create several and even tens of flows simultaniously.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=3in]{Topology}
		\caption{Topology for multi-flow tests}
		\label{fig:Topology}
	\end{center}
\end{figure}

To be clear in Figure ~\ref{fig:Topology},
we regard the server on the left (which is the receiver) as S0 and the servers on the right (which are senders) as S1 to S8.
All the servers are configured into the same virtual local area network (VLAN) so that we can construct an incast traffic inside the network.

\subsection{Switch Configuration}

Mellanox SN2700 has provided a lot of interfaces for users.
Among them, we need to configure VLAN, PFC, ECN and buffer size to make things work.

All the configurations are done according to the manual \cite{sn2700}.

\subsubsection{VLAN}
The whole expetiment environment should be separated from outside traffic including SSH connection and SFTP file transmission.
VLAN is thus needed to create a separate topology.

Switch need to enable the VLAN feature and include all machines in the same VLAN session.
This includes some work on distinguishing ports.

\subsubsection{Port mirroring}

We try to collect the InfiniBand traffic from
receiver end thus we don't need to process the received information from all different isenders.
However such method doesn't work because we can't sniffer the InfiniBand traffic from the receiver side.

Mirroring is also configured as sessions in switches.
In our experiment, we configure the session's input as the receiver and the output as another vacant machine in the same rack.
After flows are started, we start Tcpdump on the mirror session's output machine to capture packets.

Such a method can deal with small-scale incast but not large-scale ones.
Even a enlarged buffer size for Tcpdump can't handle the amount of traffic in large-scale incasts.

\subsubsection{PFC}
PFC is needed because what we want is a lossless network which is a must for data center networks.
PFC makes receiver to send PAUSE frame to sender where sending ratio is limited upon received PAUSE.
Default PFC configuration contains 8 priority classes and we only have 2 used here.
The mapping is mentioned in the following subsection about ECN.

\subsubsection{ECN}
ECN priority is configured to make CNP function well. Higher priority is needed for higher traffic class.
Inside our experiments, there are mostly two classes of traffic. RDMA traffic is in traffic class 3 and should be mapped to swith priority 3.
Other traffic like TCP is in traffic class 0 and should be mapped to switch priority 0.
The reason is that the traffic for setting up a flow shouldn't be blocked. If we don't set the priority difference, the flow setup traffic for a flow
is blocked when there are really a large number of flows. The flow setup period should use higher priority so that more flows can be created.
About this point, we explain it in later sections.
And the probability to make a ECN mark should be configured the same as mentioned in DCQCN \cite{dcqcn}.

\subsubsection{Buffer}
About the buffer size, we don't need to be very specific. The maximal buffer size inside Mellanox 2700N switch is 5.1MB.
One important factor to be considered is the buffer usage during the experiment process.
When a lot of flows are created, there must be a burst of packets in the receiver buffer at the beginning.
After a short period of reaction time, the buffer usage should go down because the senders are limited by switch based on flows.
In the results, what we are hoping to see is that the beginning of the buffer is large but after the short reaction time, the buffer is low.
Thus the buffer here is only to handle the burst at the beginning.

\subsubsection{Additional Configurations}
To make things clear, we need to cut unnecessary parts of traffic.

We obviously don't contain loops inside our topology so that the spanning tree protocol is removed from the switch.

The interface speed is set to 100Gbps when testing the hardware ability of generating CNP.
For other most experiments, interface speed is configured as 10Gbps (reason is explained in later sections).

\subsection{NIC Configuration}
At the very beginning, we need to install Mellanox OFED (driver for ConnectX-4) for every server.

After that, we have several steps (most corresponds with the upper switch configurations):
\begin{enumerate}
	\item Enable PFC and set up the priority.
	\item Enable ECN for both InfiniBand traffic and TCP traffic.
	\item Set CNP priority and DSCP priority.
	\item Set priority for RDMA traffic.
	\item Open sniffer for Tcpdump \cite{Tcpdump} to capture InfiniBand packets.
\end{enumerate}

To make all the things easier, I write Python scripts to make things automatic.
With Paramiko, we can do the remote command execution work.
I can list all the IPs needed to be configured and use Paramiko to configure them all.

The only thing tough in this is to execute a command with root.
And I use ''sh'' to get the root-necessary command executed just like this:
\begin{lstlisting}
sudo sh -c \'echo "0" > /sys/class/net/p4p2/ecn/roce_np/min_time_between_cnps\'
\end{lstlisting}

Directly executing commands on remote machines using Paramiko doesn't work because the ''sudo'' is actually running part of
the user-level commands.
''sh'' indicates the exact necessary tool needed for execution so ''sudo sh'' knows to execute with shell clearly.

\subsection{Experiment Process}

The general idea is creating an incast and observe the congestion point.
To be more specific, we are generating flows from S1~S8 (tens of flows from each) and observe the bandwidth tendency, latency and buffer usage.

However before all of that, the first experiment is to test the CNP generation speed for ConnectX-4 and ConnectX-5. 

\subsection{Ability to Generate CNP}

All NIC has limited ability. For ConnectX-4 and ConnectX-5, we hope to figure out the exact ability of doing this so that we can adjust the DCQCN efficiently.

The topology is simple. Two servers are connected to the switch, one as sender and one as receiver.
For the receiver side, the interface speed is set to 10Gbps. The sender side has an interface speed of 100Gbps.
Thus the congestion surely exists and we use Tcpdump to capture the packets during a short period of time.

Among the packets captured inside the Pcap file, we filter the traffic with right direction and right IP addresses out.
According to the timestamps, we can know something about the ability of ConnectX-4 and ConnectX-5.

\subsection{Bandwidth Testing}

Now the topology is the one shown in Figure ~\ref{fig:Topology}.
We use the command ib\_send\_bw to start flows.

Bandwidth testing needs several steps:
\begin{enumerate}
	\item Refresh machine states.
	\item Time synchronization for all servers.
	\item Start Tcpdump simultaneously on S1 to S8.
	\item Start ib\_send\_bw server on S0.
	\item Start ib\_send\_bw clients on S1 to S8.
	\item Send Pcap files to local.
	\item Parse Pcap files.
	\item Use parsed results to draw bandwidth figure.
\end{enumerate}

The overall tests is done by scripts of many languages including Python, C, Bash, Awk and Gnuplot.

Let me describe the above steps one by one.

\subsubsection{State Refresh}

Because we start a large number of flows on each machine, the command of ib\_send\_bw may fail to complete correctly, leaving
some processes continuing during the stages. We need to refresh the states for mahcines.

To kill the processes running from the last experiment, we use the command ''pkill -f'' to kill processes with the label of ''ib\_send''.

\subsubsection{Time Synchronization}
We use Network Time Protocol (NTP) to synchronize the time for S0 to S8.
The accuracy for NTP is about 0.01ms so that should work for servers on a rack.
Choosing a server inside the same rack, we set up an NTP server on it.

Then we use Python Paramiko to execute the NTP command on S0~S8 for time synchronization with the NTP server.

Here we need to claim a fact that the receiver side can't tolerate the amount of packets we need to actually generate a bandwidth figure.
Even an increased size of Tcpdump buffer can't hold the traffic of even 1 second which is obviously not enough to analyze the bandwidth tendency.
Thus we are running Tcpdump on each machine which can last for approximately 10 seconds.
With time synchronized, we can analyze packets on different machines with timestamps correct.

\subsubsection{Tcpdump Start}
Python Paramiko is still the tool used for remote command execution.

Different from normal Tcpdump, we need to specify several parameters here.
''-B 900000'' parameter is needed to increase the buffer size of Tcpdump.
''-s 60'' parameter is to capture only the first 60 bytes which contains just the packet headers.

It's hard to estimate the capturing time needed, but we can limit the packet counter of Tcpdump (which uses ''-c'').
Thus the file size is shrinked and it saves time for file transmission.

\subsubsection{ib\_send\_bw Server Client Starter}

This step contains concatenation work.
What we are doing is to accept parameters from the command line execution of the program.
This contains the concatenation of mark ''\&'' and other strings.
''\&'' is used for simultaneous command execution. Linux System usually has the ability to run over 1000 processes simultaneously.

Luckily string operations are easy for Python.
I'm actually recording the same parts of the commands and adding loops with transformed string from numbers.

About the detailed information about the commands, we have several specific options set.
The option ''-R'' is for RDMA connection setup. However we find that using RDMA traffic for a flow start causes
the traffic congested with other already-started flows. Thus we may fail to start a lot of flows at the same time.
So in real experiments, we are not using the option.

The option ''-S 3'' means the priority class for RDMA traffic. This can also be set for ROCEv2 protocol.
We add it here just to be sure that the traffic class priority is working.

The option ''-x'' denotes the device ID we are using, which can be fetched through the command ''show\_gids''.
This displays the ROCE version we are using and the corresponding device ID and destination IP.
''show\_gids'' is only needed once for each machine so that it's not actaully included as part of the automation tool.

The option ''-d'' denotes the network devices used for the execution which is mlx5\_1 for our machines.

The option ''-D'' shows the lasting time of our experiment. It counts in seconds of the continuous sending procedure.
In our experiment it's usually 60 or 80 which shows almost a complete tendency of the bandwidth and flows.

The option ''-p'' is responsible for separating flows. In our experiment, we hope to create multiple flows from one machine.
The number could be tens and even reach 90 so we need to use port number to start different flows.
The actual implementation of port allocation is like this: 

We are taking parameters of machine number and flow number of each machine at the beginning of the overall scripts.
Assume that they are $m$ and $n$.
All port number for flows starts from 10011. There is a continuous range from 10011 to $10010+m\times n$ for port numbers.
The first machine takes the position of port number like 10011, $10011+m$, ..., $10011+m\times(n-1)$.
To be more general, the $i_{th}$ flow on the $j_{th}$ machine (where $i=1,2,...,n$ and $j=1,2,...,m$) has the port number of
$10010+i+m\times(j-1)$.
Thus all the port numbers don't collide with each other. This makes later steps easier to process.
Those are mentioned in later sections.

\subsubsection{File Transmission}
To be clear, we are running experiments on 9 machines.
Besides that, we have a middle machine with accounts to login, making the machines more secure.
And we have a local machine to process the results we get. We are using a local machine to do this because
we hope to keep the experiment machines clean and fewer new tools should be used there.
I use Python Paramiko for remote login and file transmission.
A tough point is that all file location should be absolute locations but that's OK since the amount needed is not very large.

Thus the file transmission has two parts. First step would be sending Pcap files from experiment machines to the middle machine.
The second would be transmitting the files from the middle machine to the local machine.

\subsubsection{Pcap Parsing}
This contains several steps. We can't make sure that this is the easiest way to process these packets but it works fine.
\begin{enumerate}
	\item Transform Pcap files to plain text.
	\item Filter the packets to get the traffic with the receiver we want.
	\item Divide the packets from all different flows in the same machine.
	\item Collect number of packets in every millisecond.
	\item Transform timestamps to relative time starting from the beginning of the flows.
	\item Add counts of zeros for those milliseconds which contains no packets.
\end{enumerate}

For step 1, I use the command of Tcpdump with the option of ''-r'' which means reading.
With options ''-rrrr'' and ''-nnq'' we transfer the Pcap file into plain text with 8 terms.
Among them we have the timestamp, source IP, source port, destination IP and destination port.
Thus we can approximately calculate the bandwidth from such informations.

For step 2, I use Awk to do the filtering. By limiting the receiver as the only host we regard as incast receiver, we get all the receiver IP (which is the
$6^{th}$ term of the plain text) out of all possible flow receivers.

For step 3, we can only devide all the flows on the same machine using the ports.
TCP and InfiniBand has its unique port forwarding methods which would surely dismiss the ports I set for them in the ''ib\_send'' commands.
To separate all these flows, I write a C++ program to do the filtering work, collecting all the different port numbers and raise flags for new port numbers.
Each new port number starts a new file for collection, containing the packets of a single flow.
The file names are carefully calculated.
Suppose we have $m$ machines and each starts $n$ flows, then the numbering of the file names are similar to the one previously mentioned in port nubmering.
The only difference is that port numbers start from 10010 and the file names start from 85 (which is the machine index in our testbed).

For step 4, I specifically count the digits in every line (which contains a packet) of the plain text. Thus I can make sure which digits represent the condition
of the timestamps. 
Then I decide in plain text which exact digit decides the millisecond and which digit is the starting point of the timestamp.
To decide the amount of packets in one millisecond, we can find the ones with a sharing first 3 digits after the decimal point in the timestamp.
To do the filtering and counting work, we have an easy commandline tool which is ''cut''.
Using the option of ''-c'', we can have a count on the packet number with the same timetamp till millisecond digit.
We therefore print the timestamps and the packet numbers. (which has two columns and therefore can be drawn into a figure.

Step 5 and step 6 are relatively easy. 
Record the earliest packet occurs and do subtractions.
Then we need to add zeroes for those milliseconds which contains no packet to make the figure fluent and complete.

\subsubsection{Figure Plotting}

Here we use the tool of Gnuplot, which is a practical plotting tool under Linux systems.

We have figured out the timeline and packet count for each millisecond before.
For most packets, they have the same size of 1KB which is fixed by the command ''ib\_send''.
Thus we can indicate the bandwidth through packet count of each millisecond.

Gnuplot can take the number of machines and flow number of each machine as parameters.
When plotting, we need to choose data from different files with the sequenced names.

Because we don't know the number of flows we are drawing, we can't list the exact colors of lines and points.
Here I simply use numbers to choose colors from Gnuplot's color library.

Most figures shown in the results are drawn using Gnuplot.
To be more clear about the coloring condition, we can take a look at Figure ~\ref{fig:3_20_0}.

\subsection{Latency and Buffer Usage}

Most steps are similar to the last subsection about bandwidth testing.
We are doing this part because as the flow number increases bandwidth collecting is super time-consuming.
Sometimes this part may last hours and the figure we get is unclear and lines overlap each other.

Latency and buffer usage can reveal the effect of convergence more effectively.
For latency, I'm using ''ib\_send\_lat'' on each machine which provides average latency and extreme latency during a time period.
What I collect is simply the average latency which is shown in Figure ~\ref{fig:default} and Figure ~\ref{fig:improved}.

With filtered returning messages, I can gain information from the screen output.
The output should include 8 lines with each describing the result from 1 machine.
I collect the results and find some rules inside.

About buffer usages, I collect manually from interfaces provided in Mellanox switches.
I collect buffer information every 10 seconds to get them collected in Figure ~\ref{fig:default} and Figure ~\ref{fig:improved}.
All the flows last for 80 seconds and I start collecting from the $10_{th}$ second and manually read the buffer every 10 seconds.
Altogether I collect buffer usage information at 7 time points which is sufficient for us to analyze the condition.

\section{Results}

\subsection{NIC Ability to Generate CNPs}
We test the ability of generating CNPs for both ConnectX-4 and ConnectX-5 NICs.

The experiment topology is really simple.
Two hosts are connected to the Mellanox SN2700 switch, with the interface speed configured to 10Gbps and 100Gbps respectively.
The 100Gbps one then sends continuous InfiniBand traffic to the 10Gbps one.
When the switch is correctly configured with PFC and ECN enabled, we can find a burst of CNPs in a short period of time.

CNP can be distinguished by filtering the OP code of InfiniBand header which is 129.
Then I find a period of continuous CNPs and can get the approximate interval between CNPs.
With the parameter of CNP interval (min\_time\_between\_cnps) is set to 0, the results collected are just what we want.

Figure ~\ref{fig:CNPint} shows the interval of continuous CNPs under ConnectX-4 adapters.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=1in]{CNPint}
		\caption{CNP interval test}
		\label{fig:CNPint}
	\end{center}
\end{figure}

The first column marks the packet number (those ommitted are not CNPs) and the second column includes timestamps in seconds.

Tcpdump with RDMA packet sniffer on can capture enough packets to generate the results.
ConnectX-4 NIC can generate 1 packet per microsecond.
ConnectX-5 NIC can generate 3 packets per microsecond.
We can find some small improvements in hardware ability between the NICs of the two generation.
Both are capable of our DCQCN+ algorithm.

\subsection{Bandwidth Condition for Small-Scale Incast}
At the beginning, we are not sure about our switch's ability, so we conducted some small-scale incast to test the switch and our configurations.
This can be called a duplicate of the original DCQCN.
The topologies are similar to Figure ~\ref{fig:Topology} but the number of receiver hosts vary.
All the interface speeds are set to 10Gbps.

Here are the examples of several experiments.

In Figure ~\ref{fig:2on1_1}, it's a 2:1 incast with flows from 2 machines.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=6in]{2on1_1}
		\caption{2:1 incast bandwidth}
		\label{fig:2on1_1}
	\end{center}
\end{figure}

We can see, at the beginning of the flows, there's a line-rate start and the speed swiftly climbs to over 10Gbps.
After that, CNPs start working and both flows begin to limit sending rate and later within 0.5s, their speeds converge about
600 packets per millisecond, which is about 5Gbps.

In Figure ~\ref{fig:10on1_1}, it's a 10:1 incast with 10 flows coming from 10 hosts.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=6in]{10on1_1}
		\caption{10:1 incast bandwidth}
		\label{fig:10on1_1}
	\end{center}
\end{figure}

As we can see, the beginning parts are similar with 2:1 incast, but several flows fails to converge.
This is actually because of the Tcpdump covering range.
I set a limit for number of packets captured by Tcpdump for each flow and that stops the figure of showing later bandwidth.

Later we try to draw bandwidth figures for more flows on the same machine.
We use 3 machines with each sending 20 flows at the same time and we get this in Figure ~\ref{fig:3_20_0}.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=6in]{3_20_0}
		\caption{60:1 incast bandwidth}
		\label{fig:3_20_0}
	\end{center}
\end{figure}

Although I have extended the capturing range a lot, the information can be grabbed from this figure is not much.
However, it's still obvious that the 60 flows converge within 0.5 second.
We should admit that the original DCQCN works for small-scale incast.

\subsection{Latency and Buffer Usage of Large-Scale Incast}
As the flow number grows larger, we find it hard to depict the bandwidth tendency using pictures.
There are mainly 2 reasons: too many lines override each other and we can't observe the tendency clearly, 
long capturing time makes the processing and figuring time extremely long (some may even last hours).

So we transfer to use latency and queue length to take a look at the large-scale incasts.
We are sending traffic for 80 seconds and pick 7 samples for each running starting from 10 seconds after flows start.
We execute 9 experiments, with each creating 80, 160, 240, ..., 720 flows.
That is, with 8 senders, each of the senders creates 10, 20, 30, ..., 90 flows.

With default paremeters coming from DCQCN, we get Figure ~\ref{fig:default}.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=6in]{default}
		\caption{DCQCN buffer usage and latency}
		\label{fig:default}
	\end{center}
\end{figure}

Those columns marked from 86 to 95 are average latency for flows on each machine.
86 to 95 are machine numbers.
I also use scripts to collect number of flows that are successfully started to make sure that all flow start properly.

Then we apply some parameter changes to DCQCN+. To be more specific, they are listes as follows:
\begin{enumerate}
	\item DcQcnRateReduceMonitorPeriod, which the time period between rate reductions, is changed from $4\mu s$ to $40\mu s$.
	\item DcQcnTimeReset, which is the time period between rate increase events, is changed from $300\mu s$ to $3.1*flownum \mu s$.
	\item DcQcnAiRate, which is the rate increase step in Active Increase phase, is changed from $5Mbps$ to $\frac{256}{flownum} Mbps$.
\end{enumerate}
These are the only three parameters modifiable for our new algorithm and should contain more modifications if really applied to chips.

Thus we get Figure ~\ref{fig:improved}.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=6in]{improved}
		\caption{Simulated DCQCN+ buffer usage and latency}
		\label{fig:improved}
	\end{center}
\end{figure}

We can see obvious difference between Figure ~\ref{fig:improved} and Figure ~\ref{fig:default}.
It's apparent that modified parameters provides shorter latency time and less buffer usage.
That marks the success of convergence and also demands reached for datacenter networks.

To take a closer look at the difference between the original DCQCN and DCQCN+.
We get the buffer usage difference in Figure ~\ref{fig:buffer} and latency difference in Figure ~\ref{fig:latency}.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=6in]{buffer}
		\caption{Buffer usage difference between DCQCN and DCQCN+}
		\label{fig:buffer}
	\end{center}
\end{figure}

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=6in]{latency}
		\caption{Latency difference between DCQCN and DCQCN+}
		\label{fig:latency}
	\end{center}
\end{figure}

To be clear, the buffer we collected is the maximal buffer usage 5 seconds after flows are started.
And in Figure ~\ref{fig:latency} the latency is depicted in exponential scaling to make the difference clearer.

We see that at the small scales, DCQCN+ seems to be worse than DCQCN.
That's actually the reason caused by parameter scaling.
The parameter scaling provided by Mellanox interface can't handle such accurate work, so I just pick the nearest integer,
which possibly cause the small disadvantage.

From Figure ~\ref{fig:buffer} we can know that the maximal buffer allocated is 5.1MB and when the flow number reaches 720,
the convergence completely fails.
Actually for most cases if the buffer usage maintains over 200KB, that's a failure of convergence.
Most large-scale incast is over the ability of DCQCN but can be controlled by DCQCN+.

\subsection{Results from NS3 Simulations}

Simulations are conducted by Dr. Yixiao Gao. The topology used are the same with Figure ~\ref{fig:Topology}.

With 8 senders starting a same number of flows at a uniform time within 0.1 second,
we can thus collect information easier than testbeds.
Four experiments are conducted with the flow number of 800, 1200, 1600 and 2000 respectively,
which means each of the senders start 100, 150, 200, 250 flows respectively.

We get results with the link speed of 10Gbps and 40Gbps.
They are shown in Figure ~\ref{fig:simulation}.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=6in]{simulation}
		\caption{Flow rate convergence encountering large incasts in DCQCN+}
		\label{fig:simulation}
	\end{center}
\end{figure}

We can see that the results are much better than the testbed results.
That makes sense since simulation does better in details.
Simulation can actually implement the timer updates mentioned in DCQCN+ but testbed can only use static parameters and some parameters
aren't accurate.

Compared with original DCQCN, DCQCN+ has 20 times smaller queue length, which is 20 times smaller queueing latency.


\section{Conclusion and Future Work}

We have displayed the drawbacks of DCQCN.
When the network encounters large-scale traffic like an incast, DCQCN is forced to trigger PFC for a long time.
Such congestion control causes problems of high latency and buffer occupation.
We also conducted experiments to observe its features.
Then we develop DCQCN+, which does a better job in handling large-scale incasts and has similar performance with small incasts.
With adaptive parameter adjustments, DCQCN+ actually dynamicly adjust congestion control scheme according to the incoming traffic.
We can see that DCQCN+ really does much better in latency and buffer usage.

Next steps of similar work should focus more on credit-based method of congestion control schemes like \cite{credit}.
State-of-the-art methods do have better performance at the starting point.
However as the scale of traffic and data storage grows larger, bandwidth never becomes a problem.
People care more about latency and user experience.
The general case is that, the growth of bandwidth is much faster than that of processing ability and buffer size.
What we are pursuing in the future is to explore in different schemes hoping to find a mixture of method to relieve or even eliminate congestion.

\section*{Acknowledgement}

I greatly thank my advisor Prof. Chen Tian for his instructions and help all these years.
I also thank Dr. Yixiao Gao for instructing me on experiments and his great efforts on NS3 simulations.
Additionally I need to thank Bo Li and other members in NASA group for their help in switch configurations and experiment executions.
I also appreciate Huawei Co. Ltd and Mellanox for their help during the process of the project.
Thanks for the support from my family, friends and other people who give me instructions and suggestions.


\bibliographystyle{IEEEtran}
\bibliography{report}

%----------------------------------------------------------------------------------------

\section*{Appendix}

\subsection*{Codes}
This part includes my scripts.
\subsubsection*{default.py}
This is for parallel configuration for multiple machines at the same time.
\begin{lstlisting}[language=Python]
#!/usr/bin/python3
import paramiko
import threading
import time

def ssh2(ip,username,passwd,cmd):
    try:
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(ip,22,username,passwd,timeout=5)
        for m in cmd:
            print(ip,m)
            stdin, stdout, stderr = ssh.exec_command(m)
#			stdin.write("Y")
#			out = stdout.readlines()
#			for o in out:
#				print(o)
            err = stderr.readlines()
            for e in err:
                print(ip,e)
#		time.sleep(10)
#		ssh.send(chr(3))
#		print(ip,'OK')
        ssh.close()
    except:
        print(ip,'Error')


if __name__=='__main__':
    cmd3 = [
            'sudo sh -c \'echo "0" > /sys/class/net/p4p2/ecn/roce_np/min_time_between_cnps\'',

            'sudo sh -c \'echo "0" > /sys/class/net/p4p2/ecn/roce_np/enable/3\'',
            'sudo sh -c \'echo "1" > /sys/class/net/p4p2/ecn/roce_np/enable/3\'',
            'sudo sh -c \'echo "0" > /sys/class/net/p4p2/ecn/roce_rp/enable/3\'',
            'sudo sh -c \'echo "1" > /sys/class/net/p4p2/ecn/roce_rp/enable/3\'',
            'sudo sh -c \'echo "0" > /sys/class/net/p4p2/ecn/roce_np/enable/4\'',
            'sudo sh -c \'echo "1" > /sys/class/net/p4p2/ecn/roce_np/enable/4\'',
            'sudo sh -c \'echo "0" > /sys/class/net/p4p2/ecn/roce_rp/enable/4\'',
            'sudo sh -c \'echo "1" > /sys/class/net/p4p2/ecn/roce_rp/enable/4\'',

            'sudo sh -c \'echo "4" > /sys/class/net/p4p2/ecn/roce_rp/dce_tcp_g\'',
            'sudo sh -c \'echo "1" > /sys/class/net/p4p2/ecn/roce_rp/dce_tcp_rtt\'',
            'sudo sh -c \'echo "1023" > /sys/class/net/p4p2/ecn/roce_rp/initial_alpha_value\'',

            'sudo sh -c \'echo "0" > /sys/class/net/p4p2/ecn/roce_rp/rate_to_set_on_first_cnp\'',
            'sudo sh -c \'echo "50" > /sys/class/net/p4p2/ecn/roce_rp/rpg_min_dec_fac\'',
            #'sudo sh -c \'echo "1" > /sys/class/net/p4p2/ecn/roce_np/rpg_min_rate\'',
            'sudo sh -c \'echo "11" > /sys/class/net/p4p2/ecn/roce_rp/rpg_gd\'',
            'sudo sh -c \'echo "4" > /sys/class/net/p4p2/ecn/roce_rp/rate_reduce_monitor_period\'',

            'sudo sh -c \'echo "0" > /sys/class/net/p4p2/ecn/roce_rp/clamp_tgt_rate\'',
            'sudo sh -c \'echo "1" > /sys/class/net/p4p2/ecn/roce_rp/clamp_tgt_rate_after_time_inc\'',
            'sudo sh -c \'echo "300" > /sys/class/net/p4p2/ecn/roce_rp/rpg_time_reset\'',
            'sudo sh -c \'echo "32767" > /sys/class/net/p4p2/ecn/roce_rp/rpg_byte_reset\'',
            'sudo sh -c \'echo "5" > /sys/class/net/p4p2/ecn/roce_rp/rpg_threshold\'',
            'sudo sh -c \'echo "5" > /sys/class/net/p4p2/ecn/roce_rp/rpg_ai_rate\'',
            'sudo sh -c \'echo "50" > /sys/class/net/p4p2/ecn/roce_rp/rpg_hai_rate\'',

            'sudo sh -c \'echo "46" > /sys/class/net/p4p2/ecn/roce_np/cnp_dscp\'',
            'sudo sh -c \'echo "7" > /sys/class/net/p4p2/ecn/roce_np/cnp_802p_prio\'',
            ]
    username = "tian"
    passwd = "******"
    threads = []
    print("Begin......")

    for i in range(85,88):
        ip = '192.168.1.'+str(i)
        a=threading.Thread(target=ssh2,args=(ip,username,passwd,cmd3))
        a.start()

\end{lstlisting}

\subsubsection*{multiflowrun.py}
This file is the main file which contains all components for all scripts.
\begin{lstlisting}[language=Python]
#!/usr/bin/python3
import paramiko
import threading
import time
import sys
from subprocess import call

def autorun(cmd):
    try:
        call(cmd,shell=True)
    except:
        print(cmd,'error')

def ssh1(ip,username,passwd,cmd):
    try:
        ssh=paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(ip,22,username,passwd,timeout=5)
        for m in cmd:
            print(ip,m)
            stdin,stdout,stderr=ssh.exec_command(m)
            err=stderr.readlines()
            for e in err:
                print(ip,e)
            '''
            out = stdout.readlines()
            for o in out:
                outF.write(o)
                '''
        ssh.close()
    except:
        print(ip,'ssh1 Error')

def ssh2(ip,port,username,passwd,remote_file,local_file):
    try:
        transport=paramiko.Transport((ip,port))
        transport.connect(username=username,password=passwd)
        sftp=paramiko.SFTPClient.from_transport(transport)
        sftp.get(remote_file,local_file)
        sftp.close()
        transport.close()
    except:
        print(ip,'ssh2 Error')

def ssh3(ip,port,username,passwd,local_file,remote_file):
    try:
        transport=paramiko.Transport((ip,port))
        transport.connect(username=username,password=passwd)
        sftp=paramiko.SFTPClient.from_transport(transport)
        sftp.put(local_file,remote_file)
        sftp.close()
        transport.close()
    except:
        print(ip,'ssh3 Error')

if __name__=='__main__':
    outF = open('FlowCollection.txt', 'w')
    cmd0 = ['pkill -f ib_send',
            #'sudo ntpdate ntp.ubuntu.com'
            ]
    cmd1 = [
            'define',
            ]
    cmd = [
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 3 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 7 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 7 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            ]
    cmd2 = [['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa']]
    cmd3 = ['sudo tcpdump -c 1000000 -s 60 -B 900000 -i p4p2 -w ~/ycyang/result.pcap']
    cmd4 = ['/home/yangyuchen/Research/automation/multiflow/process.py ']

    mac = [86, 87, 88, 91, 92, 93, 94, 95]
    b = [i for i in range(100000)]
    j=int(sys.argv[1])
    k=int(sys.argv[2])

    for l in range(0,j+2):
        cmd2[l][0] = cmd[l][0] + str(10016+l)
        for m in range(1,k):
            cmd2[l][0] = cmd2[l][0] + ' &' + cmd[l][0] + str(10016+m*10+l)

    #for l in range(0,j):
        #print(cmd2[l][0])
    #sys.exit()


    username="tian"
    passwd="******"
    port=22
    threads=[]

    print("Phase 0")
    ip='192.168.1.85'
    b[85]=threading.Thread(target=ssh1,args=(ip,username,passwd,cmd0))
    b[85].start()
    for i in mac:
        ip='192.168.1.'+str(i)
        b[i]=threading.Thread(target=ssh1,args=(ip,username,passwd,cmd0))
        b[i].start()

    for i in mac:
        b[i].join()
    b[85].join()

    str1='for i in {1..'+str(j+2)+'}; do'
    #str2=' ib_send_bw -R -x 5 -d mlx5_1 -S 3 -D 80 -p $(('
    str2=' ib_send_bw -x 5 -d mlx5_1 -S 3 -D 80 -p $(('
    str3=' done'
    cmd1[0] = str1
    for i in range(0,k):
        cmd1[0]=cmd1[0]+str2+str(10015+10*i)+'+i))&'
    cmd1[0]=cmd1[0]+' done'
    #print(cmd2[0][0])
    #print(cmd1[0])
    #sys.exit()

    username="tian"
    passwd="******"
    port=22
    threads=[]
    print("Begin......")

    ip='192.168.1.85'
    a=threading.Thread(target=ssh1,args=(ip,username,passwd,cmd1))
    a.start()


    #time.sleep(10000000)

    time.sleep(10)
    print("Phase 2")
    for i in mac[:j]:
        ip='192.168.1.'+str(i)
        b[i]=threading.Thread(target=ssh1,args=(ip,username,passwd,cmd2[i-86]))
        b[i].start()

    #time.sleep(100)

    sys.exit()

    time.sleep(10)
    print("Phase 1")
    for i in mac[:j]:
        ip='192.168.1.'+str(i)
        b[i]=threading.Thread(target=ssh1,args=(ip,username,passwd,cmd3))
        b[i].start()


    for i in mac[:j]:
        b[i].join()

    
    print("Phase 3")
    for i in mac[:j]:
        ip='192.168.1.'+str(i)
        b[i]=threading.Thread(target=ssh2,args=(ip,port,username,passwd,'/home/tian/ycyang/result.pcap','/home/ycyang/'+str(i)+'.pcap'))

    for i in mac[:j]:
        b[i].start()

    for i in mac[:j]:
        b[i].join()
    
    ip='114.212.82.73'
    port=22
    username="yangyuchen"
    passwd="******"
    threads=[]
    print("Phase 4")
    for i in mac[:j]:
        b[i]=threading.Thread(target=ssh3,args=(ip,port,username,passwd,'/home/ycyang/'+str(i)+'.pcap','/home/yangyuchen/Research/automation/multiflow/'+str(i)+'.pcap'))

    for i in mac[:j]:
        b[i].start()

    for i in mac[:j]:
        b[i].join()

    print("Phase 5")
    cmd4[0]=cmd4[0]+str(j)+' '+str(k)
    a=threading.Thread(target=ssh1,args=(ip,username,passwd,cmd4))
    a.start()
    a.join()
    
    print("end")
\end{lstlisting}

\subsubsection*{fetchconfig.py}
This file is to fetch the original configurations for us to check on it.
\begin{lstlisting}[language=Python]
#!/usr/bin/python3
import paramiko
import threading
import time
import sys

def ssh2(ip,username,passwd,cmd):
    try:
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(ip,22,username,passwd,timeout=5)
        for m in cmd:
            stdin, stdout, stderr = ssh.exec_command(m)
            err = stderr.readlines()
            for e in err:
                print(ip,e)
            out = stdout.readlines()
            for o in out:
                print(ip,m,o)
        ssh.close()
    except:
        print(ip,'Error')


if __name__=='__main__':
    cmd = [
            'cat /sys/class/net/p4p2/ecn/roce_np/min_time_between_cnps',
            'cat /sys/class/net/p4p2/ecn/roce_np/enable/3',
            'cat /sys/class/net/p4p2/ecn/roce_rp/enable/3',
            'cat /sys/class/net/p4p2/ecn/roce_np/enable/4',
            'cat /sys/class/net/p4p2/ecn/roce_rp/enable/4',
            'cat /sys/class/net/p4p2/ecn/roce_rp/dce_tcp_g',
            'cat /sys/class/net/p4p2/ecn/roce_rp/dce_tcp_rtt',
            'cat /sys/class/net/p4p2/ecn/roce_rp/initial_alpha_value',
            'cat /sys/class/net/p4p2/ecn/roce_rp/rate_to_set_on_first_cnp',
            'cat /sys/class/net/p4p2/ecn/roce_rp/rpg_min_dec_fac',
            'cat /sys/class/net/p4p2/ecn/roce_rp/rpg_min_rate',
            'cat /sys/class/net/p4p2/ecn/roce_rp/rpg_gd',
            'cat /sys/class/net/p4p2/ecn/roce_rp/rate_reduce_monitor_period',
            'cat /sys/class/net/p4p2/ecn/roce_rp/clamp_tgt_rate',
            'cat /sys/class/net/p4p2/ecn/roce_rp/clamp_tgt_rate_after_time_inc',
            'cat /sys/class/net/p4p2/ecn/roce_rp/rpg_time_reset',
            'cat /sys/class/net/p4p2/ecn/roce_rp/rpg_byte_reset',
            'cat /sys/class/net/p4p2/ecn/roce_rp/rpg_threshold',
            'cat /sys/class/net/p4p2/ecn/roce_rp/rpg_ai_rate',
            'cat /sys/class/net/p4p2/ecn/roce_rp/rpg_hai_rate',
            'cat /sys/class/net/p4p2/ecn/roce_np/cnp_dscp',
            'cat /sys/class/net/p4p2/ecn/roce_np/cnp_802p_prio',
            ]
    username = "tian"
    passwd = "******"
    threads = []
    print("Begin......")

    i = sys.argv[1]
    ip = '192.168.1.'+i
    a=threading.Thread(target=ssh2,args=(ip,username,passwd,cmd))
    a.start()
\end{lstlisting}

\subsubsection*{multilat.py}
This file is similar to multiflowrun.py for latency and buffer tests.

\begin{lstlisting}[language=Python]
#!/usr/bin/python3
import paramiko
import threading
import time
import sys
from subprocess import call

def autorun(cmd):
    try:
        call(cmd,shell=True)
    except:
        print(cmd,'error')

def ssh1(ip,username,passwd,cmd):
    try:
        ssh=paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(ip,22,username,passwd,timeout=5)
        for m in cmd:
            print(ip,m)
            stdin,stdout,stderr=ssh.exec_command(m)
            err=stderr.readlines()
            for e in err:
                if e[0] == 'C':
                    continue
                print(ip,e)
        ssh.close()
    except:
        print(ip,'ssh1 Error')

def ssh11(ip,username,passwd,cmd):
    try:
        ssh=paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(ip,22,username,passwd,timeout=5)
        for m in cmd:
            print(ip,m)
            stdin,stdout,stderr=ssh.exec_command(m)
            out=stdout.readlines()
            for o in out:
                if o[2].isdigit() and o[3].isdigit():
                    outF.write(o)
                elif o[0].isdigit() or o[1].isdigit() or o[2].isdigit() or o[3].isdigit() or o[4].isdigit():
                    print(ip,o)
            err=stderr.readlines()
            for e in err:
                if e[0] == 'C':
                    continue
                print(ip,e)
        ssh.close()
    except:
        print(ip,'ssh11 Error')

def ssh2(ip,port,username,passwd,remote_file,local_file):
    try:
        transport=paramiko.Transport((ip,port))
        transport.connect(username=username,password=passwd)
        sftp=paramiko.SFTPClient.from_transport(transport)
        sftp.get(remote_file,local_file)
        sftp.close()
        transport.close()
    except:
        print(ip,'ssh2 Error')

def ssh3(ip,port,username,passwd,local_file,remote_file):
    try:
        transport=paramiko.Transport((ip,port))
        transport.connect(username=username,password=passwd)
        sftp=paramiko.SFTPClient.from_transport(transport)
        sftp.put(local_file,remote_file)
        sftp.close()
        transport.close()
    except:
        print(ip,'ssh3 Error')

if __name__=='__main__':
    outF = open('flowcollect.txt','w')
    cmd0 = ['pkill -f ib_send',
            #'sudo ntpdate ntp.ubuntu.com'
            ]
    cmd1 = [
            'define',
            ]
    cmd8 = [
            'define',
            ]
    cmd = [
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_bw -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 3 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 7 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 7 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            ]
    cmd6 = [
            [' ib_send_lat -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_lat -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_lat -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_lat -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_lat -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_lat -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_lat -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_lat -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_lat -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_lat -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            [' ib_send_lat -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 3 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 7 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 7 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            #[' ib_send_bw -R -S 3 -x 5 -d mlx5_1 192.168.33.85 -D 80 -p '],
            ]
    cmd2 = [['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa']]
    cmd7 = [['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa'],['aa']]
    cmd3 = ['sudo tcpdump -c 1000000 -s 60 -B 900000 -i p4p2 -w ~/ycyang/result.pcap']
    cmd4 = ['/home/yangyuchen/Research/automation/multiflow/process.py ']

    mac = [86, 87, 88, 91, 92, 93, 94, 95]
    b = [i for i in range(100000)]
    c = [i for i in range(100000)]
    j=int(sys.argv[1])
    k=int(sys.argv[2])

    for l in range(0,j+2):
        cmd2[l][0] = cmd[l][0] + str(10016+l)
        for m in range(1,k):
            cmd2[l][0] = cmd2[l][0] + ' &' + cmd[l][0] + str(10016+m*10+l)
    for l in range(0,j+2):
        cmd7[l][0] = cmd6[l][0] + str(11001+l)

    #for l in range(0,j):
        #print(cmd2[l][0])
    #sys.exit()


    username="tian"
    passwd="******"
    port=22
    threads=[]

    print("Phase 0")
    ip='192.168.1.85'
    b[85]=threading.Thread(target=ssh1,args=(ip,username,passwd,cmd0))
    b[85].start()
    for i in mac:
        ip='192.168.1.'+str(i)
        b[i]=threading.Thread(target=ssh1,args=(ip,username,passwd,cmd0))
        b[i].start()

    for i in mac:
        b[i].join()
    b[85].join()

    str1='for i in {1..'+str(j+2)+'}; do'
    str2=' ib_send_bw -x 5 -d mlx5_1 -S 3 -D 80 -p $(('
    cmd1[0] = str1
    for i in range(0,k):
        cmd1[0]=cmd1[0]+str2+str(10015+10*i)+'+i))&'
    cmd1[0]=cmd1[0]+' done'
    '''
    print(cmd2[0][0])
    print(cmd1[0])
    print(cmd8[0])
    sys.exit()
    print(cmd7[0][0])
    print(cmd7[1][0])
    '''
    str1='for i in {1..'+str(j+2)+'}; do'
    str2=' ib_send_lat -x 5 -d mlx5_1 -S 3 -D 80 -p $(('+str(11000)+'+i))&'
    cmd8[0] = str1
    cmd8[0]=cmd8[0]+str2+' done'

    username="tian"
    passwd="******"
    port=22
    threads=[]
    print("Begin......")

    ip='192.168.1.85'
    a=threading.Thread(target=ssh1,args=(ip,username,passwd,cmd1))
    a.start()

    d=threading.Thread(target=ssh1,args=(ip,username,passwd,cmd8))
    d.start()

    #time.sleep(10000000)

    time.sleep(10)
    print("Phase 2")

    for i in mac[:j]:
        ip='192.168.1.'+str(i)
        b[i]=threading.Thread(target=ssh11,args=(ip,username,passwd,cmd2[i-86]))
        c[i]=threading.Thread(target=ssh11,args=(ip,username,passwd,cmd7[i-86]))
        #time.sleep(2)
        b[i].start()
        c[i].start()

    #time.sleep(100)

    sys.exit()

    time.sleep(10)
    print("Phase 1")
    for i in mac[:j]:
        ip='192.168.1.'+str(i)
        b[i]=threading.Thread(target=ssh1,args=(ip,username,passwd,cmd3))
        b[i].start()


    for i in mac[:j]:
        b[i].join()

    
    print("Phase 3")
    for i in mac[:j]:
        ip='192.168.1.'+str(i)
        b[i]=threading.Thread(target=ssh2,args=(ip,port,username,passwd,'/home/tian/ycyang/result.pcap','/home/ycyang/'+str(i)+'.pcap'))

    for i in mac[:j]:
        b[i].start()

    for i in mac[:j]:
        b[i].join()
    
    ip='114.212.82.73'
    port=22
    username="yangyuchen"
    passwd="******"
    threads=[]
    print("Phase 4")
    for i in mac[:j]:
        b[i]=threading.Thread(target=ssh3,args=(ip,port,username,passwd,'/home/ycyang/'+str(i)+'.pcap','/home/yangyuchen/Research/automation/multiflow/'+str(i)+'.pcap'))

    for i in mac[:j]:
        b[i].start()

    for i in mac[:j]:
        b[i].join()

    print("Phase 5")
    cmd4[0]=cmd4[0]+str(j)+' '+str(k)
    a=threading.Thread(target=ssh1,args=(ip,username,passwd,cmd4))
    a.start()
    a.join()
    
    print("end")
\end{lstlisting}

\subsubsection*{process.py}
This file is the main script on local machine which contains all operations on local machine.
\begin{lstlisting}[language=Python]
#!/usr/bin/python3
import sys
import os
import threading
from subprocess import call

def autorun(cmd):
    try:
        call(cmd,shell=True)
    except:
        print(cmd,'error')

if __name__=='__main__':
    a=[i for i in range(100)]
    mac = [2, 3, 4, 5, 6]
    j=int(sys.argv[1])
    for i in mac[:j]:
        s=['tcpdump -r /home/yangyuchen/Research/automation/'+str(i)+'.pcap -s 60 -i p4p1 -tttt -nnq > /home/yangyuchen/Research/automation/'+str(i)+'.txt']
        a[i]=threading.Thread(target=autorun,args=(s))
        a[i].start()
    for i in mac[:j]:
        a[i].join()

    for i in mac[:j]:
        s=['awk \'{ if (index($6,"172.16.102.1")==1) { print $0 } }\' /home/yangyuchen/Research/automation/'+str(i)+'.txt > /home/yangyuchen/Research/automation/'+str(i)+'_data.txt']
        a[i]=threading.Thread(target=autorun,args=(s))
        a[i].start()
    for i in mac[:j]:
        a[i].join()

    for i in mac[:j]:
        s=['cut -c 12-24 /home/yangyuchen/Research/automation/'+str(i)+'_data.txt | awk \'{print $1}\' | sort | uniq -c  | awk \'{print $2 " " $1}\' > /home/yangyuchen/Research/automation/'+str(i)+'_packets_count.txt']
        a[i]=threading.Thread(target=autorun,args=(s))
        a[i].start()
    for i in mac[:j]:
        a[i].join()

    for i in mac[:j]:
        s=['/home/yangyuchen/Research/automation/reltime < /home/yangyuchen/Research/automation/'+str(i)+'_packets_count.txt > /home/yangyuchen/Research/automation/'+str(i)+'_reltime_count.txt']
        a[i]=threading.Thread(target=autorun,args=(s))
        a[i].start()
    for i in mac[:j]:
        a[i].join()

    for i in mac[:j]:
        s=['/home/yangyuchen/Research/automation/addzeros < /home/yangyuchen/Research/automation/'+str(i)+'_reltime_count.txt > /home/yangyuchen/Research/automation/'+str(i)+'_final_count.txt']
        a[i]=threading.Thread(target=autorun,args=(s))
        a[i].start()
    for i in mac[:j]:
        a[i].join()

    s=['gnuplot -c /home/yangyuchen/Research/automation/new.plot '+str(j)]
    b=threading.Thread(target=autorun,args=(s))
    b.start()
    b.join()

    print('end');
\end{lstlisting}

\subsubsection*{reltime.c}
This is to transfer absolute timestamps into relative times.
\begin{lstlisting}[language=C]
#include<stdio.h>
int main(){
	int a,b,c;
	double t0=0;
	double d,t;
	while(scanf("%d:%d:%lf %d",&a,&b,&d,&c)!=EOF){
		if(t0==0)
			t0=d+60*b+3600*a;
		t=d+60*b+3600*a;
		printf("%lf %d\n",t-t0,c);
	}
	return 0;
}
\end{lstlisting}

\subsubsection*{addzeros.c}
This is to add zeroes for those milliseconds that has no packets captured.
\begin{lstlisting}[language=C]
#include<stdio.h>
int main(){
	double a;
	double cur;
	int b;
	cur=0;
	while(scanf("%lf%d",&a,&b)!=EOF){
		if(cur!=a)
			for(;cur<a;cur+=0.001)
				printf("%lf 0\n",cur);
		printf("%lf %d\n",a,b);
		cur+=0.001;
	}
	return 0;
}
\end{lstlisting}

\subsubsection*{divide.c}
This is for division of different port numbers, i.e., division of flows on a same machine.
\begin{lstlisting}[language=C]
#include<stdio.h>
#include<string.h>
#include<stdlib.h>
int main(int argc, char** argv){
	FILE* fp[100];
	FILE* input;
	char input_file[100];
	char output_files[100][100];
	char str[100];
	int length,serverid,servernum,flownum;
	int ports[100];
	serverid=0;
	int i,l;
	int a,b,c,d,port;
	int cur=0;
	char time[100];
	l=strlen(argv[1]);
	for(i=0;i<l;i++){
		serverid*=10;
		serverid+=argv[1][i]-'0';
	}
	servernum=0;
	l=strlen(argv[2]);
	for(i=0;i<l;i++){
		servernum*=10;
		servernum+=argv[2][i]-'0';
	}
	flownum=0;
	l=strlen(argv[3]);
	for(i=0;i<l;i++){
		flownum*=10;
		flownum+=argv[3][i]-'0';
	}
	//printf("%d %d %d\n",serverid,servernum,flownum);
	strcpy(input_file,"/home/ranchyang96/Research/automation/final/");
	i=strlen(input_file);
	strcpy(input_file+i,argv[1]);
	strcpy(input_file+i+2,"_data.txt");
	input=fopen(input_file,"r");
	FILE* f=fopen("/home/ranchyang96/Research/automation/final/out","a");
	fprintf(f,"serverid:%d\tservernum:%d\tflownum:%d\n",serverid,servernum,flownum);
	while(fscanf(input,"%s",&str)!=EOF){
		fscanf(input,"%s",&time);
		fscanf(input,"%s",&str);
		fscanf(input,"%d.%d.%d.%d.%d",&a,&b,&c,&d,&port);
		fscanf(input,"%s",&str);
		fscanf(input,"%s",&str);
		fscanf(input,"%s",&str);
		fscanf(input,"%s",&str);
		if(strcmp(str,"length")!=0)
			continue;
		fscanf(input,"%d",&length);
		if(length<1000)
			continue;
		for(i=0;i<cur;i++)
			if(port==ports[i]){
				fprintf(fp[i],"%s\n",time);
				break;
			}
		if(i==cur){
			ports[cur]=port;
			if(serverid>99){
				strcpy(output_files[cur],"/home/ranchyang96/Research/automation/final/");
				i=strlen(output_files[cur]);
				output_files[cur][i]=serverid/100+'0';
				output_files[cur][i+1]=(serverid/10)%10+'0';
				output_files[cur][i+2]=serverid%10+'0';
				strcpy(output_files[cur]+i+3,"_divideddata.txt");
				fp[cur]=fopen(output_files[cur],"w");
			}
			else{
				strcpy(output_files[cur],"/home/ranchyang96/Research/automation/final/");
				i=strlen(output_files[cur]);
				output_files[cur][i]=(serverid/10)%10+'0';
				output_files[cur][i+1]=serverid%10+'0';
				strcpy(output_files[cur]+i+2,"_divideddata.txt");
				fp[cur]=fopen(output_files[cur],"w");
			}
			fprintf(fp[cur],"%s\n",time);
			cur++;
			serverid+=servernum+1;
		}
	}
	fprintf(f,"cur:%d\tfinal:%d\n",cur,serverid);
	fclose(f);

	for(i=0;i<cur;i++)
		fclose(fp[i]);
	fclose(input);
	return 0;
}
\end{lstlisting}

\subsubsection*{mul.plot}
This is for Gnuplot to draw the bandwidth figure.
\begin{lstlisting}
set title "packets/millisec"
set xlabel "reltime"
set ylabel "packets count"

set style line 1 linecolor rgb "#000000" lw 1

plot '86_reltime_count.txt' using 1:2 title "86" with line ls 1, \
	'87_reltime_count.txt' using 1:2 title "87" with line ls 2

set terminal png font "/Library/Fonts/Arial.ttf" size 3200,1800
set output "packets_count.png"

replot
\end{lstlisting}

\end{document}
